#!/bin/bash
#SBATCH --job-name=small-gidd-fineweb-4gpu-4node-lrdn-a100-64bs
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --partition=boost_usr_prod 
#SBATCH --time=03:00:00
#SBATCH --account=AIFAC_L01_028
#SBATCH --qos=normal
#SBATCH --output=slurm_logs/%x-%j.out
#SBATCH --error=slurm_logs/%x-%j.err


# Usage:
#     sbatch pretrain_gidd_leonardo.job
#

set -e  # Exit immediately on error

# Activate environment
module purge
module load python/3.11.7 nccl/2.22.3-1--gcc--12.2.0-cuda-12.2-spack0.22
export VENV_PATH=".venv"
source $VENV_PATH/bin/activate

# Distributed training
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=39591

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE 
    --nnodes $SLURM_NNODES
)

export WDIR="/leonardo_scratch/large/userexternal/avettoru/gidd_gidd_logs"

# Offline wandb logging
export WANDB_MODE=offline

# Launch training
srun torchrun "${DISTRIBUTED_ARGS[@]}" \
    --rdzv_id=$SLURM_JOBID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    gidd/train.py \
        --config-name=gidd \
        logging.run_name="$SLURM_JOB_NAME" \
        hydra.job.name="$SLURM_JOB_NAME"