#!/bin/bash
#SBATCH --job-name=small-mdlm-fineweb-4gpu-4node-lrdn-a100-64bs
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --partition=boost_usr_prod 
#SBATCH --time=03:00:00
#SBATCH --account=AIFAC_L01_028
#SBATCH --qos=normal
#SBATCH --output=slurm_logs/%x-%j.out
#SBATCH --error=slurm_logs/%x-%j.err

# For quick tests on max 2 nodes and max 30 min walltime: #SBATCH --qos=boost_qos_dbg
# FOr tests on max 64 nodes and 24h walltime: #SBATCH --qos=normal
# For large node numbers (> 64 nodes): add #SBATCH --qos=boost_qos_bprod

set -e  # Exit immediately on error

# Activate environment
module purge
module load python/3.11.7 nccl/2.22.3-1--gcc--12.2.0-cuda-12.2-spack0.22
export VENV_PATH=".venv"
source $VENV_PATH/bin/activate

# Distributed training
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=39591

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE 
    --nnodes $SLURM_NNODES
)

# files are deleted after 40 days
export WDIR="/leonardo_scratch/large/userexternal/avettoru/gidd_mdlm_logs"
#export WANDB_DIR="/leonardo_work/AIFAC_L01_028/avettoru/gidd/"

# to run wandb in offline mode
export WANDB_MODE=offline

# Launch training
srun torchrun "${DISTRIBUTED_ARGS[@]}" \
  --rdzv_id=$SLURM_JOBID \
  --rdzv_backend=c10d \
  --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
  --master_addr=$MASTER_ADDR \
  --master_port=$MASTER_PORT \
  gidd/train.py \
    --config-name=mdlm \
    logging.run_name="$SLURM_JOB_NAME" \
    hydra.job.name="$SLURM_JOB_NAME"