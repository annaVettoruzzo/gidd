#!/bin/bash
#SBATCH --job-name=1B-mdlm-fineweb-4gpu-4node-snls-a100-8bs
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --partition=gpu_a100
#SBATCH --time=3:00:00
#SBATCH --output=slurm_logs/%x-%j.out
#SBATCH --error=slurm_logs/%x-%j.err

# Activate environment
module purge
module load 2024 Python/3.12.3-GCCcore-13.3.0 NCCL/2.22.3-GCCcore-13.3.0-CUDA-12.6.0
export VENV_PATH=".venv"
source $VENV_PATH/bin/activate

# Distributed training
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=39591

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE 
    --nnodes $SLURM_NNODES
)

# files are deleted after 40 days
export WDIR="/scratch-shared/avettoruzzo/gidd_mdlm_logs"

# Launch training
srun torchrun "${DISTRIBUTED_ARGS[@]}" \
  --rdzv_id=$SLURM_JOBID \
  --rdzv_backend=c10d \
  --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
  --master_addr=$MASTER_ADDR \
  --master_port=$MASTER_PORT \
  gidd/train.py \
    --config-name=mdlm \
    logging.run_name="$SLURM_JOB_NAME" \
    hydra.job.name="$SLURM_JOB_NAME"